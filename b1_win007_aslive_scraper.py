#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çƒæ¢ç½‘å…¨åŠŸèƒ½æ•°æ®çˆ¬è™«å·¥å…·
æ”¯æŒï¼šè®©çƒæ•°æ®ã€å¤§å°çƒæ•°æ®ã€å¯¹é˜µåˆ†ææ•°æ®æŠ“å–
"""

import os
import sys
import time
import csv
import random
import argparse
import pandas as pd
from datetime import datetime
from typing import List, Tuple, Dict, Set
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import re


class TitanFullScraper:
    """çƒæ¢ç½‘å…¨åŠŸèƒ½æ•°æ®çˆ¬è™«ç±»"""
    
    # å…¬å¸IDåˆ—è¡¨
    COMPANY_IDS = [1, 3, 8, 12, 14, 17, 22, 23, 24, 31, 35]
    
    def __init__(self, headless=True, delay_range=(1, 1)):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            delay_range: è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’)
        """
        self.headless = headless
        self.delay_range = delay_range
        self.driver = None
        self.match_ids = []
        
    def load_match_ids_from_csv(self, csv_file):
        """
        ä»CSVæ–‡ä»¶åŠ è½½match_idåˆ—è¡¨
        
        Args:
            csv_file: CSVæ–‡ä»¶è·¯å¾„
            
        Returns:
            match_idåˆ—è¡¨
        """
        try:
            df = pd.read_csv(csv_file, encoding='utf-8-sig')
            if 'match_id' not in df.columns:
                print(f"âŒ CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°'match_id'åˆ—")
                return []
            
            # æå–å”¯ä¸€çš„match_id
            match_ids = df['match_id'].dropna().astype(int).unique().tolist()
            print(f"âœ“ ä» {csv_file} åŠ è½½äº† {len(match_ids)} ä¸ªå”¯ä¸€çš„ match_id")
            return match_ids
        except Exception as e:
            print(f"âŒ åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _init_driver(self):
        """åˆå§‹åŒ–Selenium WebDriverï¼Œé…ç½®åçˆ¬ç­–ç•¥"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument('--headless')
        
        # åçˆ¬è™«é…ç½®
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        # éšæœºUser-Agent
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        chrome_options.add_argument(f'user-agent={random.choice(user_agents)}')
        
        # ç¦ç”¨è‡ªåŠ¨åŒ–æ ‡è¯†
        chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            # ä¿®æ”¹webdriverå±æ€§
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    })
                '''
            })
            print("âœ“ WebDriveråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–WebDriverå¤±è´¥: {e}")
            print("è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriver")
            sys.exit(1)
    
    def _random_delay(self):
        """éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        delay = random.uniform(*self.delay_range)
        time.sleep(delay)
    
    def _format_score(self, score_text):
        """
        æ ¼å¼åŒ–æ¯”åˆ†ä¸º[m:n]æ ¼å¼
        
        Args:
            score_text: åŸå§‹æ¯”åˆ†æ–‡æœ¬
            
        Returns:
            æ ¼å¼åŒ–åçš„æ¯”åˆ†ï¼Œå¦‚"[1:2]"
        """
        if not score_text or score_text == '-':
            return '-'
        
        # å¤„ç†å„ç§å¯èƒ½çš„åˆ†éš”ç¬¦
        for sep in ['-', ':', 'ï¼', 'ï¼š']:
            if sep in score_text:
                parts = score_text.split(sep)
                if len(parts) == 2:
                    try:
                        m, n = parts[0].strip(), parts[1].strip()
                        return f"[{m}:{n}]"
                    except:
                        pass
        
        return score_text
    
    def scrape_handicap(self, match_id, company_id):
        """
        æŠ“å–äºšæ´²è®©çƒæ•°æ®
        
        Args:
            match_id: æ¯”èµ›ID
            company_id: å…¬å¸ID
            
        Returns:
            æ•°æ®åˆ—è¡¨
        """
        url = f"https://vip.titan007.com/changeDetail/handicap.aspx?id={match_id}&companyid={company_id}&l=0"
        
        try:
            self.driver.get(url)
            self._random_delay()
            
            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "table"))
            )
            
            # è·å–é¡µé¢HTMLå¹¶è§£æ
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æŸ¥æ‰¾æ•°æ®è¡¨æ ¼
            table = soup.find('table', {'id': 'table1'}) or soup.find('table', {'class': 'tbl01'}) or soup.find('table')
            
            if not table:
                return []
            
            data = []
            rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
            
            for row in rows:
                cols = row.find_all(['td', 'th'])
                if len(cols) >= 7:
                    time_str = cols[0].get_text(strip=True)
                    score = self._format_score(cols[1].get_text(strip=True))
                    home_odds = cols[2].get_text(strip=True)
                    handicap = cols[3].get_text(strip=True)
                    away_odds = cols[4].get_text(strip=True)
                    change_time = cols[5].get_text(strip=True)
                    status = cols[6].get_text(strip=True) if len(cols) > 6 else ''
                    
                    data.append([
                        match_id, company_id, time_str, score, home_odds, 
                        handicap, away_odds, change_time, status
                    ])
            
            return data
            
        except TimeoutException:
            print(f"âš ï¸  Match {match_id}, Company {company_id}: é¡µé¢åŠ è½½è¶…æ—¶")
            return []
        except Exception as e:
            print(f"âŒ Match {match_id}, Company {company_id}: {e}")
            return []
    
    def scrape_overunder(self, match_id, company_id):
        """
        æŠ“å–å¤§å°çƒæ•°æ®
        
        Args:
            match_id: æ¯”èµ›ID
            company_id: å…¬å¸ID
            
        Returns:
            æ•°æ®åˆ—è¡¨
        """
        url = f"https://vip.titan007.com/changeDetail/overunder.aspx?id={match_id}&companyid={company_id}&l=0"
        
        try:
            self.driver.get(url)
            self._random_delay()
            
            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "table"))
            )
            
            # è·å–é¡µé¢HTMLå¹¶è§£æ
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # æŸ¥æ‰¾æ•°æ®è¡¨æ ¼
            table = soup.find('table', {'id': 'table1'}) or soup.find('table', {'class': 'tbl01'}) or soup.find('table')
            
            if not table:
                return []
            
            data = []
            rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
            
            for row in rows:
                cols = row.find_all(['td', 'th'])
                if len(cols) >= 7:
                    time_str = cols[0].get_text(strip=True)
                    score = self._format_score(cols[1].get_text(strip=True))
                    over = cols[2].get_text(strip=True)
                    line = cols[3].get_text(strip=True)
                    under = cols[4].get_text(strip=True)
                    change_time = cols[5].get_text(strip=True)
                    status = cols[6].get_text(strip=True) if len(cols) > 6 else ''
                    
                    data.append([
                        match_id, company_id, time_str, score, over, 
                        line, under, change_time, status
                    ])
            
            return data
            
        except TimeoutException:
            print(f"âš ï¸  Match {match_id}, Company {company_id}: é¡µé¢åŠ è½½è¶…æ—¶")
            return []
        except Exception as e:
            print(f"âŒ Match {match_id}, Company {company_id}: {e}")
            return []
    
    def scrape_analysis(self, match_id, output_dir='matches_ana'):
        """
        æŠ“å–å¯¹é˜µåˆ†ææ•°æ®
        
        Args:
            match_id: æ¯”èµ›ID
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            (å¤´éƒ¨æ•°æ®, æ˜¯å¦æˆåŠŸ)
        """
        url = f"https://zq.titan007.com/analysis/{match_id}cn.htm"
        
        try:
            self.driver.get(url)
            self._random_delay()
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ===== è§£æå¤´éƒ¨ä¿¡æ¯ =====
            header_data = self._parse_analysis_header(soup, match_id)
            
            # ===== è§£æè¯¦ç»†æ•°æ®å¹¶ä¿å­˜ =====
            self._parse_and_save_analysis_details(soup, match_id, output_dir)
            
            return header_data, True
            
        except Exception as e:
            print(f"âŒ Match {match_id} åˆ†æé¡µé¢æŠ“å–å¤±è´¥: {e}")
            return None, False
    
    def _parse_analysis_header(self, soup, match_id):
        """è§£æå¯¹é˜µåˆ†æé¡µé¢çš„å¤´éƒ¨ä¿¡æ¯"""
        try:
            header_data = {
                'match_id': match_id,
                'home_team_id': '',
                'home_team': '',
                'score': '',
                'away_team_id': '',
                'away_team': '',
                'weather': '',
                'temperature': ''
            }
            
            # æå–ä¸»é˜Ÿä¿¡æ¯ï¼ˆä»é“¾æ¥ä¸­æå–IDï¼‰
            home_link = soup.find('a', href=re.compile(r'/cn/team/Summary/\d+\.html'))
            if home_link:
                match = re.search(r'/cn/team/Summary/(\d+)\.html', home_link['href'])
                if match:
                    header_data['home_team_id'] = match.group(1)
                header_data['home_team'] = home_link.get_text(strip=True)
            
            # æå–å®¢é˜Ÿä¿¡æ¯
            away_links = soup.find_all('a', href=re.compile(r'/cn/team/Summary/\d+\.html'))
            if len(away_links) >= 2:
                away_link = away_links[1]
                match = re.search(r'/cn/team/Summary/(\d+)\.html', away_link['href'])
                if match:
                    header_data['away_team_id'] = match.group(1)
                header_data['away_team'] = away_link.get_text(strip=True)
            
            # æå–æ¯”åˆ†
            score_elem = soup.find('strong', class_='cred')
            if score_elem:
                header_data['score'] = self._format_score(score_elem.get_text(strip=True))
            
            # æå–å¤©æ°”å’Œæ¸©åº¦
            weather_div = soup.find('div', class_='weather')
            if weather_div:
                weather_text = weather_div.get_text(strip=True)
                # è§£æå¤©æ°”å’Œæ¸©åº¦
                if 'â„ƒ' in weather_text:
                    parts = weather_text.split('â„ƒ')
                    if len(parts) >= 2:
                        header_data['temperature'] = parts[0].strip() + 'â„ƒ'
                        header_data['weather'] = parts[1].strip()
            
            return header_data
            
        except Exception as e:
            print(f"âš ï¸  è§£æå¤´éƒ¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_and_save_analysis_details(self, soup, match_id, output_dir):
        """è§£æå¹¶ä¿å­˜è¯¦ç»†åˆ†ææ•°æ®"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            Path(output_dir).mkdir(parents=True, exist_ok=True)
            
            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
            tables = soup.find_all('table')
            
            analysis_data = {
                'è”èµ›ç§¯åˆ†æ’å': [],
                'æ•°æ®å¯¹æ¯”_è¿‘10åœº': [],
                'é˜µå®¹æƒ…å†µ': [],
                'å¯¹èµ›å¾€ç»©': [],
                'è¿‘æœŸæˆ˜ç»©_ä¸»é˜Ÿ': [],
                'è¿‘æœŸæˆ˜ç»©_å®¢é˜Ÿ': [],
                'è”èµ›ç›˜è·¯èµ°åŠ¿': [],
                'ç›¸åŒç›˜è·¯': [],
                'å…¥çƒæ•°_ä¸Šä¸‹åŠåœºå…¥çƒåˆ†å¸ƒ': [],
                'åŠå…¨åœº': [],
                'è¿›çƒæ•°_å•åŒ': [],
                'è¿›çƒæ—¶é—´': [],
                'æœ¬èµ›å­£æ•°æ®ç»Ÿè®¡å¯¹æ¯”': []
            }
            
            # éå†æ‰€æœ‰è¡¨æ ¼ï¼Œå°è¯•è¯†åˆ«å’Œæå–æ•°æ®
            for idx, table in enumerate(tables):
                rows = table.find_all('tr')
                if not rows:
                    continue
                
                # æå–è¡¨æ ¼æ•°æ®
                table_data = []
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    row_data = [col.get_text(strip=True) for col in cols]
                    if row_data:
                        table_data.append(row_data)
                
                # æ ¹æ®è¡¨å¤´å†…å®¹åˆ¤æ–­è¡¨æ ¼ç±»å‹
                if table_data:
                    header = ' '.join(table_data[0])
                    
                    if 'ç§¯åˆ†' in header or 'æ’å' in header:
                        analysis_data['è”èµ›ç§¯åˆ†æ’å'].extend(table_data)
                    elif 'è¿‘10åœº' in header or 'æœ€è¿‘' in header:
                        analysis_data['æ•°æ®å¯¹æ¯”_è¿‘10åœº'].extend(table_data)
                    elif 'é˜µå®¹' in header or 'é¦–å‘' in header:
                        analysis_data['é˜µå®¹æƒ…å†µ'].extend(table_data)
                    elif 'å¾€ç»©' in header or 'äº¤é”‹' in header:
                        analysis_data['å¯¹èµ›å¾€ç»©'].extend(table_data)
                    elif 'ç›˜è·¯' in header:
                        analysis_data['è”èµ›ç›˜è·¯èµ°åŠ¿'].extend(table_data)
                    elif 'å…¥çƒ' in header or 'è¿›çƒåˆ†å¸ƒ' in header:
                        analysis_data['å…¥çƒæ•°_ä¸Šä¸‹åŠåœºå…¥çƒåˆ†å¸ƒ'].extend(table_data)
                    elif 'åŠå…¨åœº' in header:
                        analysis_data['åŠå…¨åœº'].extend(table_data)
                    elif 'å•åŒ' in header:
                        analysis_data['è¿›çƒæ•°_å•åŒ'].extend(table_data)
            
            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            output_file = os.path.join(output_dir, f'{match_id}_analysis_data.csv')
            
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                for section_name, section_data in analysis_data.items():
                    if section_data:
                        writer.writerow([f'=== {section_name} ==='])
                        writer.writerows(section_data)
                        writer.writerow([])  # ç©ºè¡Œåˆ†éš”
            
            print(f"  âœ“ è¯¦ç»†åˆ†ææ•°æ®å·²ä¿å­˜: {output_file}")
            
        except Exception as e:
            print(f"âš ï¸  è§£æè¯¦ç»†æ•°æ®å¤±è´¥: {e}")
    
    def batch_scrape_handicap(self, match_ids, company_ids, analysis_dir):
        """æ‰¹é‡æŠ“å–è®©çƒæ•°æ®"""
        print("\n" + "="*60)
        print("å¼€å§‹æŠ“å–äºšæ´²è®©çƒæ•°æ®")
        print("="*60)
        print(f"Match IDs: {len(match_ids)} ä¸ª")
        print(f"Company IDs: {company_ids}")
        
        self._init_driver()
        
        total = len(match_ids) * len(company_ids)
        processed = 0
        total_records = 0
        
        for match_id in match_ids:
            for company_id in company_ids:
                output_file = analysis_dir + '/' + str(match_id) + '_handicap_live_data_cp' + str(company_id) + '.csv'
                f = open(output_file, 'w', newline='', encoding='utf-8-sig')
                writer = csv.writer(f)
                writer.writerow(['match_id', 'company_id', 'æ—¶é—´', 'æ¯”åˆ†', 'ä¸»é˜Ÿ', 'ç›˜å£', 'å®¢é˜Ÿ', 'å˜åŒ–æ—¶é—´', 'çŠ¶æ€'])

                processed += 1

                data = self.scrape_handicap(match_id, company_id)
                
                if data:
                    writer.writerows(data)
                    total_records += len(data)
                
                progress = (processed / total) * 100
                print(f"è¿›åº¦: {processed}/{total} ({progress:.1f}%) | è®°å½•æ•°: {total_records}", end='\r')
                
                f.close()
                print(f"ğŸ’¾ æ–‡ä»¶: {output_file}")

                self._random_delay()
        
        self.driver.quit()
        print(f"\n\nâœ… è®©çƒæ•°æ®æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records}")
        
    
    def batch_scrape_overunder(self, match_ids, company_ids, analysis_dir):
        """æ‰¹é‡æŠ“å–å¤§å°çƒæ•°æ®"""
        print("\n" + "="*60)
        print("å¼€å§‹æŠ“å–å¤§å°çƒæ•°æ®")
        print("="*60)
        print(f"Match IDs: {len(match_ids)} ä¸ª")
        print(f"Company IDs: {company_ids}")
        
        self._init_driver()

            
        total = len(match_ids) * len(company_ids)
        processed = 0
        total_records = 0
        
        for match_id in match_ids:
            for company_id in company_ids:
                output_file = analysis_dir + '/' + str(match_id) + '_overunder_live_data_cp' +str(company_id)+ '.csv'
                f = open(output_file, 'w', newline='', encoding='utf-8-sig')
                writer = csv.writer(f)
                writer.writerow(['match_id', 'company_id', 'æ—¶é—´', 'æ¯”åˆ†', 'å¤§çƒ', 'ç›˜å£', 'å°çƒ', 'å˜åŒ–æ—¶é—´', 'çŠ¶æ€'])
                processed += 1
                data = self.scrape_overunder(match_id, company_id)
                
                if data:
                    writer.writerows(data)
                    total_records += len(data)
                
                progress = (processed / total) * 100
                print(f"è¿›åº¦: {processed}/{total} ({progress:.1f}%) | è®°å½•æ•°: {total_records}", end='\r')
                
                print(f"ğŸ’¾ æ–‡ä»¶: {output_file}")
                f.close()
                self._random_delay()
            
        
        self.driver.quit()
        print(f"\n\nâœ… å¤§å°çƒæ•°æ®æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records}")
    
    def batch_scrape_analysis(self, match_ids, daily_str, daily_dir, analysis_dir):
        """æ‰¹é‡æŠ“å–å¯¹é˜µåˆ†ææ•°æ®"""
        print("\n" + "="*60)
        print("å¼€å§‹æŠ“å–å¯¹é˜µåˆ†ææ•°æ®")
        print("="*60)
        print(f"Match IDs: {len(match_ids)} ä¸ª")
        
        self._init_driver()
        
        # åˆ›å»ºè¯¦ç»†æ•°æ®è¾“å‡ºç›®å½•
        #Path(detail_dir).mkdir(parents=True, exist_ok=True)

        output_file = daily_dir + '/' + daily_str + '_matches_header.csv'
        
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['match_id', 'ä¸»é˜ŸID', 'ä¸»é˜Ÿ', 'æ¯”åˆ†', 'å®¢é˜ŸID', 'å®¢é˜Ÿ', 'å¤©æ°”', 'æ¸©åº¦'])
            
            processed = 0
            success_count = 0
            
            for match_id in match_ids:
                processed += 1
                header_data, success = self.scrape_analysis(match_id, analysis_dir)
                
                if success and header_data:
                    writer.writerow([
                        header_data['match_id'],
                        header_data['home_team_id'],
                        header_data['home_team'],
                        header_data['score'],
                        header_data['away_team_id'],
                        header_data['away_team'],
                        header_data['weather'],
                        header_data['temperature']
                    ])
                    success_count += 1
                
                progress = (processed / len(match_ids)) * 100
                print(f"è¿›åº¦: {processed}/{len(match_ids)} ({progress:.1f}%) | æˆåŠŸ: {success_count}", end='\r')
                
                self._random_delay()
        
        self.driver.quit()
        print(f"\n\nâœ… å¯¹é˜µåˆ†ææ•°æ®æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“Š æˆåŠŸæŠ“å–: {success_count}/{len(match_ids)}")
        print(f"ğŸ’¾ å¤´éƒ¨æ•°æ®: {output_file}")
        print(f"ğŸ’¾ è¯¦ç»†æ•°æ®: {analysis_dir}/")
    
    def batch_scrape_all(self, match_ids, company_ids, daily_str, daily_dir, analysis_dir):
        """æ‰¹é‡æŠ“å–æ‰€æœ‰æ•°æ®"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print("\n" + "="*60)
        print("æ‰¹é‡æŠ“å–æ‰€æœ‰æ•°æ®")
        print("="*60)
        print(f"Match IDs: {len(match_ids)} ä¸ª")
        print(f"Company IDs: {company_ids}")
        
        # 1. æŠ“å–è®©çƒæ•°æ®
        # handicap_file = f'{daily_str}_handicap_live_data.csv'
        self.batch_scrape_handicap(match_ids, company_ids, analysis_dir)
        
        print("\n" + "-"*60 + "\n")
        
        # 2. æŠ“å–å¤§å°çƒæ•°æ®
        # overunder_file = f'{daily_str}_overunder_live_data.csv'
        self.batch_scrape_overunder(match_ids, company_ids, analysis_dir)
        
        print("\n" + "-"*60 + "\n")
        
        # 3. æŠ“å–å¯¹é˜µåˆ†ææ•°æ®
        # self.batch_scrape_analysis(match_ids, daily_str, daily_dir, analysis_dir)
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ•°æ®æŠ“å–å®Œæˆï¼")
        print("="*60)


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='çƒæ¢ç½‘äºšç›˜æ•°æ®çˆ¬è™«å·¥å…· (è®©çƒ+å¤§å°çƒ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä»CSVåŠ è½½match_idï¼ŒæŠ“å–è®©çƒå’Œå¤§å°çƒæ•°æ®
  python b1_win007_asias2d_scraper.py --csv matches.csv --output-dir data/win007

  # åªæŠ“å–è®©çƒæ•°æ®
  python b1_win007_asias2d_scraper.py --csv matches.csv --type handicap --output-dir data/win007

  # åªæŠ“å–å¤§å°çƒæ•°æ®
  python b1_win007_asias2d_scraper.py --csv matches.csv --type overunder --output-dir data/win007

  # è‡ªå®šä¹‰å…¬å¸ID
  python b1_win007_asias2d_scraper.py --csv matches.csv --companies 1 3 8 --output-dir data/win007
        """
    )

    parser.add_argument(
        '--csv',
        required=True,
        help='åŒ…å«match_idçš„CSVæ–‡ä»¶è·¯å¾„'
    )

    parser.add_argument(
        '--type',
        choices=['handicap', 'overunder', 'all'],
        default='all',
        help='æŠ“å–æ•°æ®ç±»å‹ (é»˜è®¤: allï¼ŒæŠ“å–è®©çƒ+å¤§å°çƒ)'
    )

    parser.add_argument(
        '--companies',
        type=int,
        nargs='+',
        default=[3, 8],
        help='å…¬å¸IDåˆ—è¡¨ (é»˜è®¤: 8)'
    )

    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='é™åˆ¶å¤„ç†çš„match_idæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰'
    )

    parser.add_argument(
        '--output-dir',
        type=str,
        default='data/win007',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: data/win007)'
    )

    parser.add_argument(
        '--no-headless',
        action='store_true',
        help='æ˜¾ç¤ºæµè§ˆå™¨çª—å£'
    )

    parser.add_argument(
        '--delay',
        type=float,
        nargs=2,
        default=[1, 1],
        metavar=('MIN', 'MAX'),
        help='è¯·æ±‚å»¶è¿ŸèŒƒå›´(ç§’) (é»˜è®¤: 1 1)'
    )

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "="*60)
    print("çƒæ¢ç½‘äºšç›˜æ•°æ®çˆ¬è™«å·¥å…· (è®©çƒ+å¤§å°çƒ)")
    print("="*60)
    print(f"CSVæ–‡ä»¶: {args.csv}")
    print(f"æŠ“å–ç±»å‹: {args.type}")
    print(f"å…¬å¸IDs: {args.companies}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è¿è¡Œæ¨¡å¼: {'æ˜¾ç¤ºæµè§ˆå™¨' if args.no_headless else 'æ— å¤´æ¨¡å¼'}")
    print(f"è¯·æ±‚å»¶è¿Ÿ: {args.delay[0]}-{args.delay[1]}ç§’")
    print("="*60)

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = TitanFullScraper(
        headless=not args.no_headless,
        delay_range=tuple(args.delay)
    )

    # åŠ è½½match_id
    match_ids = scraper.load_match_ids_from_csv(args.csv)

    if not match_ids:
        print("âŒ æœªèƒ½åŠ è½½ä»»ä½•match_idï¼Œç¨‹åºé€€å‡º")
        sys.exit(1)

    # é™åˆ¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if args.limit:
        match_ids = match_ids[:args.limit]
        print(f"âš ï¸  é™åˆ¶å¤„ç†å‰ {args.limit} ä¸ªmatch_id")

    try:
        if args.type == 'handicap':
            scraper.batch_scrape_handicap(match_ids, args.companies, args.output_dir)
        elif args.type == 'overunder':
            scraper.batch_scrape_overunder(match_ids, args.companies, args.output_dir)
        else:  # all
            scraper.batch_scrape_handicap(match_ids, args.companies, args.output_dir)
            print("\n" + "-"*60 + "\n")
            scraper.batch_scrape_overunder(match_ids, args.companies, args.output_dir)
            print("\n" + "="*60)
            print("æ‰€æœ‰æ•°æ®æŠ“å–å®Œæˆï¼")
            print("="*60)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        if scraper.driver:
            scraper.driver.quit()
        sys.exit(0)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        if scraper.driver:
            scraper.driver.quit()
        sys.exit(1)


if __name__ == '__main__':
    main()
